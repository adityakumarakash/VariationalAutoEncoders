{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(143)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 5449903.57it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 259161.33it/s]                           \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 3086172.94it/s]                            \n",
      "8192it [00:00, 106689.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "testset = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Inspect one image from the dataset\n",
    "img, label = trainset.__getitem__(0)\n",
    "print(img.size())\n",
    "\n",
    "def imshow(img):\n",
    "    # Expects an img in tensor -1 to 1 range\n",
    "    #img = (img + 1)/2.0\n",
    "    npimg = img.squeeze().numpy()\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "imshow(img)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for dataset\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "latent_dim = 2\n",
    "hidden_dim = 400\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        mu = self.fc21(x)\n",
    "        logvar = self.fc22(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Creates the decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 784)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Creates the VAE class\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def reparam(self, mu, logvar):\n",
    "        sigma = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + eps*sigma\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function involved\n",
    "def loss_func(x, recon_x, mu, logvar):\n",
    "    #recon_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    recon_loss = F.mse_loss(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KL_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + KL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training process\n",
    "def train(epoch, model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = model(data)\n",
    "        loss = loss_func(data, recon, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train_epoch: {}\\t[{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx*len(data), len(train_loader.dataset), \n",
    "                loss.item()/len(data)))\n",
    "    print('Epoch: {} Avg train loss = {:.6f}'.format(\n",
    "        epoch, train_loss/len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test process\n",
    "def test(epoch, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        recon, mu, logvar = model(data)\n",
    "        loss = loss_func(data, recon, mu, logvar)\n",
    "        test_loss += loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Epoch: {} Avg test loss: {:.6f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_epoch: 1\t[0/60000]\tLoss: 184.444321\n",
      "Train_epoch: 1\t[25600/60000]\tLoss: 48.524063\n",
      "Train_epoch: 1\t[51200/60000]\tLoss: 47.369255\n",
      "Epoch: 1 Avg train loss = 50.131027\n",
      "Epoch: 1 Avg test loss: 44.327679\n",
      "Train_epoch: 2\t[0/60000]\tLoss: 41.602646\n",
      "Train_epoch: 2\t[25600/60000]\tLoss: 44.463203\n",
      "Train_epoch: 2\t[51200/60000]\tLoss: 44.423939\n",
      "Epoch: 2 Avg train loss = 43.211666\n",
      "Epoch: 2 Avg test loss: 42.671375\n",
      "Train_epoch: 3\t[0/60000]\tLoss: 40.014095\n",
      "Train_epoch: 3\t[25600/60000]\tLoss: 43.573746\n",
      "Train_epoch: 3\t[51200/60000]\tLoss: 43.580910\n",
      "Epoch: 3 Avg train loss = 42.196541\n",
      "Epoch: 3 Avg test loss: 41.923019\n",
      "Train_epoch: 4\t[0/60000]\tLoss: 38.840454\n",
      "Train_epoch: 4\t[25600/60000]\tLoss: 42.447224\n",
      "Train_epoch: 4\t[51200/60000]\tLoss: 42.963448\n",
      "Epoch: 4 Avg train loss = 41.638470\n",
      "Epoch: 4 Avg test loss: 41.465099\n",
      "Train_epoch: 5\t[0/60000]\tLoss: 38.280609\n",
      "Train_epoch: 5\t[25600/60000]\tLoss: 41.961849\n",
      "Train_epoch: 5\t[51200/60000]\tLoss: 42.397339\n",
      "Epoch: 5 Avg train loss = 41.205856\n",
      "Epoch: 5 Avg test loss: 41.212566\n",
      "Train_epoch: 6\t[0/60000]\tLoss: 38.279354\n",
      "Train_epoch: 6\t[25600/60000]\tLoss: 41.400139\n",
      "Train_epoch: 6\t[51200/60000]\tLoss: 42.075630\n",
      "Epoch: 6 Avg train loss = 40.876865\n",
      "Epoch: 6 Avg test loss: 40.894508\n",
      "Train_epoch: 7\t[0/60000]\tLoss: 37.416359\n",
      "Train_epoch: 7\t[25600/60000]\tLoss: 41.056652\n",
      "Train_epoch: 7\t[51200/60000]\tLoss: 41.418095\n",
      "Epoch: 7 Avg train loss = 40.570591\n",
      "Epoch: 7 Avg test loss: 40.549648\n",
      "Train_epoch: 8\t[0/60000]\tLoss: 36.872772\n",
      "Train_epoch: 8\t[25600/60000]\tLoss: 40.755276\n",
      "Train_epoch: 8\t[51200/60000]\tLoss: 41.247864\n",
      "Epoch: 8 Avg train loss = 40.316360\n",
      "Epoch: 8 Avg test loss: 40.357861\n",
      "Train_epoch: 9\t[0/60000]\tLoss: 37.470856\n",
      "Train_epoch: 9\t[25600/60000]\tLoss: 40.728951\n",
      "Train_epoch: 9\t[51200/60000]\tLoss: 40.655632\n",
      "Epoch: 9 Avg train loss = 40.092781\n",
      "Epoch: 9 Avg test loss: 40.216866\n",
      "Train_epoch: 10\t[0/60000]\tLoss: 36.615589\n",
      "Train_epoch: 10\t[25600/60000]\tLoss: 40.259338\n",
      "Train_epoch: 10\t[51200/60000]\tLoss: 40.578571\n",
      "Epoch: 10 Avg train loss = 39.877247\n",
      "Epoch: 10 Avg test loss: 39.973949\n",
      "Train_epoch: 11\t[0/60000]\tLoss: 36.225800\n",
      "Train_epoch: 11\t[25600/60000]\tLoss: 39.858196\n",
      "Train_epoch: 11\t[51200/60000]\tLoss: 40.428974\n",
      "Epoch: 11 Avg train loss = 39.694809\n",
      "Epoch: 11 Avg test loss: 39.758144\n",
      "Train_epoch: 12\t[0/60000]\tLoss: 36.295235\n",
      "Train_epoch: 12\t[25600/60000]\tLoss: 39.755829\n",
      "Train_epoch: 12\t[51200/60000]\tLoss: 39.850403\n",
      "Epoch: 12 Avg train loss = 39.531181\n",
      "Epoch: 12 Avg test loss: 39.775169\n",
      "Train_epoch: 13\t[0/60000]\tLoss: 35.954514\n",
      "Train_epoch: 13\t[25600/60000]\tLoss: 39.946590\n",
      "Train_epoch: 13\t[51200/60000]\tLoss: 40.564449\n",
      "Epoch: 13 Avg train loss = 39.409866\n",
      "Epoch: 13 Avg test loss: 39.596165\n",
      "Train_epoch: 14\t[0/60000]\tLoss: 36.000816\n",
      "Train_epoch: 14\t[25600/60000]\tLoss: 39.523624\n",
      "Train_epoch: 14\t[51200/60000]\tLoss: 39.772102\n",
      "Epoch: 14 Avg train loss = 39.269135\n",
      "Epoch: 14 Avg test loss: 39.564503\n",
      "Train_epoch: 15\t[0/60000]\tLoss: 35.602264\n",
      "Train_epoch: 15\t[25600/60000]\tLoss: 39.566879\n",
      "Train_epoch: 15\t[51200/60000]\tLoss: 39.436909\n",
      "Epoch: 15 Avg train loss = 39.155849\n",
      "Epoch: 15 Avg test loss: 39.663460\n",
      "Train_epoch: 16\t[0/60000]\tLoss: 36.091621\n",
      "Train_epoch: 16\t[25600/60000]\tLoss: 39.248581\n",
      "Train_epoch: 16\t[51200/60000]\tLoss: 39.404213\n",
      "Epoch: 16 Avg train loss = 39.071789\n",
      "Epoch: 16 Avg test loss: 39.480160\n",
      "Train_epoch: 17\t[0/60000]\tLoss: 35.646366\n",
      "Train_epoch: 17\t[25600/60000]\tLoss: 39.079243\n",
      "Train_epoch: 17\t[51200/60000]\tLoss: 39.686039\n",
      "Epoch: 17 Avg train loss = 38.932289\n",
      "Epoch: 17 Avg test loss: 39.370770\n",
      "Train_epoch: 18\t[0/60000]\tLoss: 35.764835\n"
     ]
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "model = nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "else:\n",
    "    print('Directory', results_dir, 'already exists')\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train(epoch, model, optimizer, train_loader)\n",
    "    test(epoch, model, test_loader)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, latent_dim).to(device)\n",
    "        sample = model.module.decoder(sample).cpu()\n",
    "        sample_filename = os.path.join(results_dir, 'sample_' + str(epoch) + '.png')\n",
    "        save_image(sample.view(64, 1, 28, 28), sample_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of images for hidden dim = 2\n",
    "with torch.no_grad():\n",
    "    n = 32\n",
    "    sample = torch.linspace(-1.0, 1.0, steps=n)\n",
    "    x, y = torch.meshgrid(sample, sample)\n",
    "    x = x.unsqueeze(2)\n",
    "    y = y.unsqueeze(2)\n",
    "    sample = torch.cat((x, y), 2)\n",
    "    sample = sample.view(-1, 2).to(device)\n",
    "    sample = model.module.decoder(sample).cpu()\n",
    "    sample_filename = os.path.join(results_dir, 'final' + '.png')\n",
    "    save_image(sample.view(n*n, 1, 28, 28), sample_filename, nrow=n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
